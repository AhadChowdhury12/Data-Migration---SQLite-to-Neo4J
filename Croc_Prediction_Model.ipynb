{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtRMsLB0WriC6du4sIZXhW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhadChowdhury12/Data-Migration---SQLite-to-Neo4J/blob/main/Croc_Prediction_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEf3SCSqNNH6"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "-QCH229hNndV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "\n",
        "# Load the crocodile sightings data (first dataset)\n",
        "croc_df = pd.read_excel('Crocodile_Survey_Data_2021_22.xlsx')  # Replace with your actual CSV file path\n",
        "\n",
        "# Load the zone data (second dataset, shapefile)\n",
        "zone_gdf = gpd.read_file('NT_Croc_Capture_Zones.shp')  # Replace with the actual shapefile path\n",
        "\n",
        "# Convert the latitude and longitude in the crocodile data into Point geometries\n",
        "# Create GeoDataFrame for crocodile sightings\n",
        "croc_gdf = gpd.GeoDataFrame(\n",
        "    croc_df,\n",
        "    geometry=gpd.points_from_xy(croc_df['Longitude'], croc_df['Latitude__']),\n",
        "    crs=\"EPSG:4326\"  # Assuming WGS 84 as the coordinate system\n",
        ")\n",
        "\n",
        "# Ensure the CRS of both datasets are the same\n",
        "if croc_gdf.crs != zone_gdf.crs:\n",
        "    croc_gdf = croc_gdf.to_crs(zone_gdf.crs)\n",
        "\n",
        "# Perform a spatial join to assign the ZONENAME from the second dataset (zones) to the sightings based on location\n",
        "croc_with_zone = gpd.sjoin(\n",
        "    croc_gdf,\n",
        "    zone_gdf[['ZONENAME', 'geometry']],\n",
        "    how=\"left\",\n",
        "    predicate=\"within\"  # Updated from 'op' to 'predicate'\n",
        ")\n",
        "\n",
        "# Now, 'ZONENAME' from zone_gdf is in croc_with_zone['ZONENAME']\n",
        "\n",
        "# Drop latitude, longitude, and geometry columns, keeping ZONENAME\n",
        "columns_to_drop = ['geometry', 'Latitude__', 'Longitude']\n",
        "croc_with_zone = croc_with_zone.drop(columns=columns_to_drop)\n",
        "\n",
        "# Keep relevant columns in the final dataframe\n",
        "final_croc_df = croc_with_zone[['Species_ORIGINAL', 'Species', 'Size (feet)', 'Position', 'UTC_Date', 'ZONENAME']]\n",
        "\n",
        "# Save the updated dataset to a new CSV file\n",
        "final_croc_df.to_csv('crocodile_sightings_with_zones.csv', index=False)\n",
        "\n",
        "print(\"Data saved to 'crocodile_sightings_with_zones.csv'\")\n"
      ],
      "metadata": {
        "id": "lgsWoSoKNnj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_croc_df\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "df = final_croc_df\n",
        "df = df[df['ZONENAME'].isin([0, 1])]\n",
        "\n",
        "\n",
        "# Step 1: Preprocess the data\n",
        "# Convert 'UTC_Date' to datetime and extract day, month, year\n",
        "df['UTC_Date'] = pd.to_datetime(df['UTC_Date'])\n",
        "df['Day'] = df['UTC_Date'].dt.day\n",
        "df['Month'] = df['UTC_Date'].dt.month\n",
        "df['Year'] = df['UTC_Date'].dt.year\n",
        "\n",
        "# Convert categorical columns to numerical (using LabelEncoder)\n",
        "label_encoder = LabelEncoder()\n",
        "df['Species'] = label_encoder.fit_transform(df['Species'])\n",
        "df['Position'] = label_encoder.fit_transform(df['Position'])\n",
        "df['ZONENAME'] = label_encoder.fit_transform(df['ZONENAME'])  # Encode 'Management Zone' and 'Outside Management Zone'\n",
        "\n",
        "# Drop 'UTC_Date' and 'Species_ORIGINAL' columns (not needed)\n",
        "df = df.drop(columns=['UTC_Date', 'Species_ORIGINAL'])\n",
        "\n",
        "# Step 2: Define features (X) and target (y)\n",
        "X = df[['Species', 'Size (feet)', 'Position', 'Day', 'Month', 'Year']]\n",
        "y = df['ZONENAME']\n",
        "\n",
        "# Step 3: Split the data into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Train a Random Forest Classifier\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate the model (using accuracy score)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Show predictions vs actual values\n",
        "predictions = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
        "print(predictions)\n",
        "\n",
        "# Optional: Save the trained model for future use\n",
        "# import joblib\n",
        "# joblib.dump(model, 'management_zone_prediction_model.pkl')\n",
        "\n",
        "print(\"Model training complete.\")\n"
      ],
      "metadata": {
        "id": "P0_AUd3QNntR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Management Zone', 'Outside Management Zone'], yticklabels=['Management Zone', 'Outside Management Zone'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "civo2ApCNnx_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}